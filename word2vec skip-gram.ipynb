{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4L7tyqwdAHdpen206USw3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#The preprocessing steps, forward pass, and output probabilities.\n"],"metadata":{"id":"Gfe1UW179CHp"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXM0iP_zyTbd","executionInfo":{"status":"ok","timestamp":1716121186406,"user_tz":-300,"elapsed":1521,"user":{"displayName":"Faiqa Rashid","userId":"08894191857955636443"}},"outputId":"72e29a89-4887-47de-bc51-3c3873d34920"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training pairs: [('the', 'weather'), ('weather', 'the'), ('weather', 'is'), ('is', 'weather'), ('is', 'windy'), ('windy', 'is')]\n","Input word: weather\n","Target word: the\n","Predicted probabilities: [0.32459592 0.24121418 0.15888091 0.27530899]\n","Target index: 0\n","Probability of target word being 'the': 0.32459592098276485\n","Input word: the, Target word: weather, Probability: 0.2184\n","Input word: weather, Target word: the, Probability: 0.3246\n","Input word: weather, Target word: is, Probability: 0.1589\n","Input word: is, Target word: weather, Probability: 0.2265\n","Input word: is, Target word: windy, Probability: 0.2894\n","Input word: windy, Target word: is, Probability: 0.2358\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["# implementation of the Word2Vec Skip-Gram model for the given corpus \"the weather is windy\" in Python using NumPy.\n","\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Corpus and preprocessing\n","corpus = \"the weather is windy\"\n","words = corpus.split()\n","\n","# Create vocabulary and word index mapping\n","vocab = list(set(words))\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for i, word in enumerate(vocab)}\n","\n","# One-hot encoding\n","onehot_encoder = OneHotEncoder(sparse=False)\n","onehot_encoder.fit(np.array(vocab).reshape(-1, 1))\n","\n","# Function to generate training pairs (skip-gram model)\n","def generate_training_data(corpus, window_size):\n","    words = corpus.split()\n","    training_pairs = []\n","    for i, word in enumerate(words):\n","        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n","            if i != j:\n","                training_pairs.append((word, words[j]))\n","    return training_pairs\n","\n","# Generate training data\n","window_size = 1\n","training_pairs = generate_training_data(corpus, window_size)\n","print(\"Training pairs:\", training_pairs)\n","\n","# Initialize parameters\n","vocab_size = len(vocab)\n","embedding_dim = 2\n","W = np.random.rand(vocab_size, embedding_dim)\n","W_prime = np.random.rand(embedding_dim, vocab_size)\n","\n","# Forward pass function\n","def forward(input_word):\n","    x = onehot_encoder.transform([[input_word]])\n","    h = np.dot(W.T, x.T).reshape(-1)\n","    u = np.dot(W_prime.T, h)\n","    y_pred = np.exp(u) / np.sum(np.exp(u))\n","    return y_pred\n","\n","# Forward pass for a specific pair (weather, the)\n","input_word = \"weather\"\n","target_word = \"the\"\n","\n","y_pred = forward(input_word)\n","target_idx = word_to_idx[target_word]\n","\n","print(\"Input word:\", input_word)\n","print(\"Target word:\", target_word)\n","print(\"Predicted probabilities:\", y_pred)\n","print(\"Target index:\", target_idx)\n","print(\"Probability of target word being 'the':\", y_pred[target_idx])\n","\n","# Run forward pass for all training pairs and print results\n","for input_word, target_word in training_pairs:\n","    y_pred = forward(input_word)\n","    target_idx = word_to_idx[target_word]\n","    print(f\"Input word: {input_word}, Target word: {target_word}, Probability: {y_pred[target_idx]:.4f}\")"]},{"cell_type":"markdown","source":["### Explanation of the Code\n","1. **Corpus and Preprocessing**: The corpus is split into words. A vocabulary and mapping from words to indices are created.\n","2. **One-Hot Encoding**: The words are one-hot encoded using `OneHotEncoder` from `sklearn`.\n","3. **Generate Training Data**: A function generates training pairs using a context window size.\n","4. **Initialize Parameters**: Random weights for `W` (input-to-hidden) and `W'` (hidden-to-output) matrices are initialized.\n","5. **Forward Pass Function**: The forward pass calculates the hidden layer activations `h`, output layer pre-activations `u`, and softmax probabilities `y_pred`.\n","6. **Forward Pass for Specific Pair**: The code performs a forward pass for the pair (\"weather\", \"the\") and prints the predicted probabilities.\n","7. **Loop Over Training Pairs**: The forward pass is run for all training pairs to print the predicted probabilities for each target word.\n","\n","This code provides a basic implementation of the Skip-Gram model and demonstrates the forward propagation step for the given corpus."],"metadata":{"id":"7rD1aSaYywAI"}},{"cell_type":"markdown","source":["# Forward Pass + Backward Pass"],"metadata":{"id":"RYYGuPYI9LRe"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Corpus and preprocessing\n","corpus = \"the weather is windy\"\n","words = corpus.split()\n","\n","# Create vocabulary and word index mapping\n","vocab = list(set(words))\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for i, word in enumerate(vocab)}\n","\n","# One-hot encoding\n","onehot_encoder = OneHotEncoder(sparse_output=False)\n","onehot_encoder.fit(np.array(vocab).reshape(-1, 1))\n","\n","# Function to generate training pairs (skip-gram model)\n","def generate_training_data(corpus, window_size):\n","    words = corpus.split()\n","    training_pairs = []\n","    for i, word in enumerate(words):\n","        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n","            if i != j:\n","                training_pairs.append((word, words[j]))\n","    return training_pairs\n","\n","# Generate training data\n","window_size = 1\n","training_pairs = generate_training_data(corpus, window_size)\n","print(\"Training pairs:\", training_pairs)\n","\n","# Initialize parameters\n","vocab_size = len(vocab)\n","embedding_dim = 2\n","W = np.random.randn(vocab_size, embedding_dim)\n","W_prime = np.random.randn(embedding_dim, vocab_size)\n","learning_rate = 0.01\n","\n","# Forward and backward pass functions\n","def forward(input_word):\n","    x = onehot_encoder.transform([[input_word]]).reshape(-1)\n","    h = np.dot(W.T, x)\n","    u = np.dot(W_prime.T, h)\n","    y_pred = np.exp(u) / np.sum(np.exp(u))\n","    return x, h, u, y_pred\n","\n","def backward(x, h, y_pred, target_word_idx):\n","    # One-hot encoding of the target word\n","    y_true = np.zeros(vocab_size)\n","    y_true[target_word_idx] = 1\n","\n","    # Calculate error\n","    e = y_pred - y_true\n","\n","    # Gradients\n","    dW_prime = np.outer(h, e)\n","    dh = np.dot(W_prime, e)\n","    dW = np.outer(x, dh)\n","\n","    return dW, dW_prime\n","\n","# Training the model\n","for epoch in range(1000):\n","    for input_word, target_word in training_pairs:\n","        x, h, u, y_pred = forward(input_word)\n","        target_word_idx = word_to_idx[target_word]\n","        dW, dW_prime = backward(x, h, y_pred, target_word_idx)\n","\n","        # Update weights\n","        W -= learning_rate * dW\n","        W_prime -= learning_rate * dW_prime\n","\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch} completed\")\n","\n","# Check the trained embeddings\n","for word in vocab:\n","    idx = word_to_idx[word]\n","    print(f\"Word: {word}, Embedding: {W[idx]}\")\n","\n","# Run forward pass for all training pairs and print results\n","for input_word, target_word in training_pairs:\n","    _, _, _, y_pred = forward(input_word)\n","    target_idx = word_to_idx[target_word]\n","    print(f\"Input word: {input_word}, Target word: {target_word}, Probability: {y_pred[target_idx]:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rm-VavtG8IeS","executionInfo":{"status":"ok","timestamp":1716123775465,"user_tz":-300,"elapsed":5777,"user":{"displayName":"Faiqa Rashid","userId":"08894191857955636443"}},"outputId":"444bedb7-4361-4add-8153-31f5e79bdb89"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Training pairs: [('the', 'weather'), ('weather', 'the'), ('weather', 'is'), ('is', 'weather'), ('is', 'windy'), ('windy', 'is')]\n","Epoch 0 completed\n","Epoch 100 completed\n","Epoch 200 completed\n","Epoch 300 completed\n","Epoch 400 completed\n","Epoch 500 completed\n","Epoch 600 completed\n","Epoch 700 completed\n","Epoch 800 completed\n","Epoch 900 completed\n","Word: the, Embedding: [-0.59357182 -0.75520178]\n","Word: weather, Embedding: [-2.19560021 -1.1263265 ]\n","Word: is, Embedding: [0.88790898 1.30578802]\n","Word: windy, Embedding: [ 2.20447211 -0.01149224]\n","Input word: the, Target word: weather, Probability: 0.8956\n","Input word: weather, Target word: the, Probability: 0.4860\n","Input word: weather, Target word: is, Probability: 0.5087\n","Input word: is, Target word: weather, Probability: 0.6151\n","Input word: is, Target word: windy, Probability: 0.3522\n","Input word: windy, Target word: is, Probability: 0.9825\n"]}]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **Corpus and Preprocessing**: The corpus is split into words. A vocabulary and mappings from words to indices are created.\n","2. **One-Hot Encoding**: The words are one-hot encoded using `OneHotEncoder` from `sklearn`.\n","3. **Generate Training Data**: A function generates training pairs using a context window size.\n","4. **Initialize Parameters**: Random weights for `W` (input-to-hidden) and `W'` (hidden-to-output) matrices are initialized.\n","5. **Forward Pass Function**: The forward pass calculates the hidden layer activations `h`, output layer pre-activations `u`, and softmax probabilities `y_pred`.\n","6. **Backward Pass Function**: The backward pass calculates the gradients of the loss with respect to the weights `W` and `W'`.\n","7. **Training Loop**: The model is trained for a number of epochs using stochastic gradient descent. The weights `W` and `W'` are updated using the calculated gradients.\n","8. **Check Trained Embeddings**: The trained embeddings for each word in the vocabulary are printed.\n","9. **Run Forward Pass for All Training Pairs**: The forward pass is run for all training pairs to print the predicted probabilities for each target word.\n","\n","This code provides a basic implementation of training a Skip-Gram model using forward and backward propagation steps."],"metadata":{"id":"jMXn8kKM8NI4"}}]}