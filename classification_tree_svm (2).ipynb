{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIyF8gaOPeXG"
      },
      "source": [
        "# **Credit Card Fraud Detection using Scikit-Learn and Snap ML**\n",
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBR4WmVqPeXM"
      },
      "source": [
        "## Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57vBXdq3PeXN"
      },
      "source": [
        "- Perform basic data preprocessing in Python\n",
        "- Model a classification task using the Scikit-Learn and Snap ML Python APIs\n",
        "- Train Suppport Vector Machine and Decision Tree models using Scikit-Learn and Snap ML\n",
        "- Run inference and assess the quality of the trained models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-zz_KNxPeXN"
      },
      "source": [
        "## Table of Contents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBsgFQEhPeXN"
      },
      "source": [
        "<div>\n",
        "    <ol>\n",
        "        <li><a href=\"#introduction\">Introduction</a></li>\n",
        "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
        "        <li><a href=\"#dataset_analysis\">Dataset Analysis</a></li>\n",
        "        <li><a href=\"#dataset_preprocessing\">Dataset Preprocessing</a></li>\n",
        "        <li><a href=\"#dataset_split\">Dataset Train/Test Split</a></li>\n",
        "        <li><a href=\"#dt_sklearn\">Build a Decision Tree Classifier model with Scikit-Learn</a></li>\n",
        "        <li><a href=\"#dt_snap\">Build a Decision Tree Classifier model with Snap ML</a></li>\n",
        "        <li><a href=\"#dt_sklearn_snap\">Evaluate the Scikit-Learn and Snap ML Decision Tree Classifiers</a></li>\n",
        "        <li><a href=\"#svm_sklearn\">Build a Support Vector Machine model with Scikit-Learn</a></li>\n",
        "        <li><a href=\"#svm_snap\">Build a Support Vector Machine model with Snap ML</a></li>\n",
        "        <li><a href=\"#svm_sklearn_snap\">Evaluate the Scikit-Learn and Snap ML Support Vector Machine Models</a></li>\n",
        "    </ol>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZgCa-XKPeXN"
      },
      "source": [
        "-------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9-JAgUXPeXO"
      },
      "source": [
        "<div id=\"Introduction\">\n",
        "    <h2>Introduction</h2>\n",
        "    <br>Imagine that you work for a financial institution and part of your job is to build a model that predicts if a credit card transaction is fraudulent or not. You can model the problem as a binary classification problem. A transaction belongs to the positive class (1) if it is a fraud, otherwise it belongs to the negative class (0).\n",
        "    <br>\n",
        "    <br>You have access to transactions that occured over a certain period of time. The majority of the transactions are normally legitimate and only a small fraction are non-legitimate. Thus, typically you have access to a dataset that is highly unbalanced. This is also the case of the current dataset: only 492 transactions out of 284,807 are fraudulent (the positive class - the frauds - accounts for 0.172% of all transactions).\n",
        "    <br>\n",
        "    <br>To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset.\n",
        "    <br>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-DyNRRCZPeXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a5c5f9-3705-47c2-c331-97e3a5c829ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.6)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.10)\n"
          ]
        }
      ],
      "source": [
        "# install the opendatasets package\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-7M6_GIFPeXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ab5583-3e7c-4259-886f-09071b25693a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: rumaizakosar\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
            "Downloading creditcardfraud.zip to ./creditcardfraud\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.0M/66.0M [00:00<00:00, 86.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "# downloadinf the dataset (this is a Kaggle dataset)\n",
        "# during download input of Kaggle username and password is required\n",
        "od.download(\"https://www.kaggle.com/mlg-ulb/creditcardfraud\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fic9S3BMPeXQ"
      },
      "source": [
        "<div id=\"import_libraries\">\n",
        "    <h2>Import Libraries</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tmGZbOVXPeXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff8134f-64bc-4730-9725-8d477e6a5ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snapml\n",
            "  Downloading snapml-1.16.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from snapml) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from snapml) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from snapml) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->snapml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->snapml) (3.5.0)\n",
            "Downloading snapml-1.16.1-cp310-cp310-manylinux_2_28_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snapml\n",
            "Successfully installed snapml-1.16.1\n"
          ]
        }
      ],
      "source": [
        "# Snap ML is available on PyPI. To install it simply run the pip command below.\n",
        "!pip install snapml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hricGfb7PeXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989f4904-be19-4a68-cc4c-a95e30d824ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas, scikit-learn, matplotlib\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.2.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "langchain 0.3.11 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.0 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.0 which is incompatible.\n",
            "snapml 1.16.1 requires numpy<2.0.0,>=1.21.3; python_version == \"3.10\", but you have numpy 2.2.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.10.0 numpy-2.2.0 pandas-2.2.3 scikit-learn-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scikit-learn numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnAr2WAqQfN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4167b5-eede-43de-e655-8a930563ecec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m233.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (2.2.0)\n",
            "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.0 which is incompatible.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.0 which is incompatible.\n",
            "snapml 1.16.1 requires numpy<2.0.0,>=1.21.3; python_version == \"3.10\", but you have numpy 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Zv-FdGQq5m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "7033b035-92b2-42ea-8417-80d58b078df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.0\n",
            "    Uninstalling numpy-2.2.0:\n",
            "      Successfully uninstalled numpy-2.2.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "24f1959f1028438abff7cc2ef0b8c2b5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade numpy==1.24.3 scipy==1.10.1 scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0030ehiiPeXR"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we need to use in this lab\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3_yT1dPeXR"
      },
      "source": [
        "<div id=\"dataset_analysis\">\n",
        "    <h2>Dataset Analysis</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_8DrDf0PeXR"
      },
      "source": [
        "In this section you will read the dataset in a Pandas dataframe and visualize its content. You will also look at some data statistics.\n",
        "\n",
        "Note: A Pandas dataframe is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure. For more information: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32jOfTQfPeXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f9151ed9-50b6-49d6-ad60-fe69269f6f31"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-32b8328a68f9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'creditcardfraud/creditcard.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" observations in the credit card fraud dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" variables in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# read the input data\n",
        "raw_data = pd.read_csv('creditcardfraud/creditcard.csv')\n",
        "print(\"There are \" + str(len(raw_data)) + \" observations in the credit card fraud dataset.\")\n",
        "print(\"There are \" + str(len(raw_data.columns)) + \" variables in the dataset.\")\n",
        "\n",
        "# display the first rows in the dataset\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkIUom7APeXR"
      },
      "source": [
        "In practice, a financial institution may have access to a much larger dataset of transactions. To simulate such a case, we will inflate the original one 10 times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI9Fb7f7PeXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0241d21d-43dc-480f-c62c-2a5ebc8d3556"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'raw_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-82c3560b6adf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# inflate the original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbig_raw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_replicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_raw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" observations in the inflated credit card fraud dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
          ]
        }
      ],
      "source": [
        "n_replicas = 10\n",
        "\n",
        "# inflate the original dataset\n",
        "big_raw_data = pd.DataFrame(np.repeat(raw_data.values, n_replicas, axis=0), columns=raw_data.columns)\n",
        "\n",
        "print(\"There are \" + str(len(big_raw_data)) + \" observations in the inflated credit card fraud dataset.\")\n",
        "print(\"There are \" + str(len(big_raw_data.columns)) + \" variables in the dataset.\")\n",
        "\n",
        "# display first rows in the new dataset\n",
        "big_raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC_ZPJogPeXR"
      },
      "source": [
        "Each row in the dataset represents a credit card transaction. As shown above, each row has 31 variables. One variable (the last variable in the table above) is called Class and represents the target variable. Your objective will be to train a model that uses the other variables to predict the value of the Class variable. Let's first retrieve basic statistics about the target variable.\n",
        "\n",
        "Note: For confidentiality reasons, the original names of most features are anonymized V1, V2 .. V28. The values of these features are the result of a PCA transformation and are numerical. The feature 'Class' is the target variable and it takes two values: 1 in case of fraud and 0 otherwise. For more information about the dataset please visit this webpage: https://www.kaggle.com/mlg-ulb/creditcardfraud.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_d9XN1DPeXS"
      },
      "outputs": [],
      "source": [
        "labels = big_raw_data.Class.unique()  # get the set of distinct classes\n",
        "sizes = big_raw_data.Class.value_counts().values  # get the count of each class\n",
        "\n",
        "plt.pie(sizes,\n",
        "        labels=labels,\n",
        "        autopct='%1.3f%%',\n",
        "        shadow=True,\n",
        "        explode=[0.2,0],\n",
        "        colors=['c', 'b'])\n",
        "plt.title('Target Variable Value Counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0mjAQv8PeXS"
      },
      "source": [
        "As shown above, the Class variable has two values: 0 (the credit card transaction is legitimate) and 1 (the credit card transaction is fraudulent). Thus, you need to _model a binary classification problem_. Moreover, the dataset is highly unbalanced, the target variable classes are not represented equally. This case requires special attention when training or when evaluating the quality of a model. One way of handing this case at train time is to bias the model to pay more attention to the samples in the minority class. The models under the current study will be configured to take into account the class weights of the samples at train/fit time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNordNsPeXS"
      },
      "source": [
        "The credit card transactions have different amounts. Here we plot a histogram that shows the distribution of these amounts. Print the range of these amounts (min/max) and the 90th percentile of the amount values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GubE6VBPeXT"
      },
      "outputs": [],
      "source": [
        "hist = big_raw_data.Amount.hist(alpha=0.4, color='green', edgecolor='black', bins=range(0,1000,100))\n",
        "plt.title('Distribution of Amounts')\n",
        "\n",
        "print(\"Minimum amount value is \", np.min(big_raw_data.Amount.values))\n",
        "print(\"Maximum amount value is \", np.max(big_raw_data.Amount.values))\n",
        "print(\"90% of the transactions have an amount less or equal than \", np.percentile(raw_data.Amount.values, 90))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-5aLuAPrPeXT"
      },
      "outputs": [],
      "source": [
        "plt.hist(big_raw_data.Amount.values, 6, histtype='bar', facecolor='g', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "print(\"Minimum amount value is \", np.min(big_raw_data.Amount.values))\n",
        "print(\"Maximum amount value is \", np.max(big_raw_data.Amount.values))\n",
        "print(\"90% of the transactions have an amount less or equal than \", np.percentile(raw_data.Amount.values, 90))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oQpvdmMPeXT"
      },
      "source": [
        "<div id=\"dataset_preprocessing\">\n",
        "    <h2>Dataset Preprocessing</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB6Pl9nyPeXT"
      },
      "source": [
        "In this subsection the data for training is prepared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZat37tPeXT"
      },
      "outputs": [],
      "source": [
        "# data preprocessing such as scaling/normalization is typically useful for linear models to accelerate the training convergence\n",
        "\n",
        "# standardize features by removing the mean and scaling to unit variance\n",
        "big_raw_data.iloc[:, 1:30] = StandardScaler().fit_transform(big_raw_data.iloc[:, 1:30])\n",
        "data_matrix = big_raw_data.values\n",
        "\n",
        "# X: feature matrix (for this analysis, we exclude the Time variable from the dataset)\n",
        "X = data_matrix[:, 1:30]\n",
        "\n",
        "# y: labels vector\n",
        "y = data_matrix[:, 30]\n",
        "\n",
        "# data normalization\n",
        "X = normalize(X, norm=\"l1\")\n",
        "\n",
        "# print the shape of the features matrix and the labels vector\n",
        "print('X.shape=', X.shape, 'y.shape=', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEOjzW97PeXT"
      },
      "source": [
        "<div id=\"dataset_split\">\n",
        "    <h2>Dataset Train/Test Split</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxAotZlXPeXT"
      },
      "source": [
        "Now that the dataset is ready for building the classification models, we will first divide the pre-processed dataset into a subset to be used for training the model (the train set) and a subset to be used for evaluating the quality of the model (the test set).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJosDMt_PeXU"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape)\n",
        "print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77K1eZv-PeXU"
      },
      "source": [
        "<div id=\"dt_sklearn\">\n",
        "    <h2>Build a Decision Tree Classifier model with Scikit-Learn</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecjo-5_GPeXU"
      },
      "outputs": [],
      "source": [
        "# compute the sample weights to be used as input to the train routine so that it takes into account the class imbalance present in this dataset\n",
        "w_train = compute_sample_weight('balanced', y_train)\n",
        "\n",
        "# import the Decision Tree Classifier Model from scikit-learn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
        "sklearn_dt = DecisionTreeClassifier(max_depth=4, random_state=35)\n",
        "\n",
        "# train a Decision Tree Classifier using scikit-learn\n",
        "t0 = time.time()\n",
        "sklearn_dt.fit(X_train, y_train, sample_weight=w_train)\n",
        "sklearn_time = time.time()-t0\n",
        "print(\"[Scikit-Learn] Training time (s):  {0:.5f}\".format(sklearn_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk61cB-lPeXU"
      },
      "source": [
        "<div id=\"dt_snapml\">\n",
        "    <h2>Build a Decision Tree Classifier model with Snap ML</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zTt2wzzPeXU"
      },
      "outputs": [],
      "source": [
        "# if not already computed, compute the sample weights to be used as input to the train routine so that\n",
        "# it takes into account the class imbalance present in this dataset w_train = compute_sample_weight('balanced', y_train)\n",
        "\n",
        "# import the Decision Tree Classifier Model from Snap ML\n",
        "from snapml import DecisionTreeClassifier\n",
        "\n",
        "# Snap ML offers multi-threaded CPU/GPU training of decision trees, unlike scikit-learn\n",
        "# to use the GPU, set the use_gpu parameter to True\n",
        "# snapml_dt = DecisionTreeClassifier(max_depth=4, random_state=45, use_gpu=True)\n",
        "\n",
        "# to set the number of CPU threads used at training time, set the n_jobs parameter\n",
        "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
        "snapml_dt = DecisionTreeClassifier(max_depth=4, random_state=45, n_jobs=4)\n",
        "\n",
        "# train a Decision Tree Classifier model using Snap ML\n",
        "t0 = time.time()\n",
        "snapml_dt.fit(X_train, y_train, sample_weight=w_train)\n",
        "snapml_time = time.time()-t0\n",
        "print(\"[Snap ML] Training time (s):  {0:.5f}\".format(snapml_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRUvBehVPeXU"
      },
      "source": [
        "<div id=\"dt_sklearn_snapml\">\n",
        "    <h2>Evaluate the Scikit-Learn and Snap ML Decision Tree Classifier Models</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GFeO53EPeXU"
      },
      "outputs": [],
      "source": [
        "# Snap ML vs Scikit-Learn training speedup\n",
        "training_speedup = sklearn_time/snapml_time\n",
        "print('[Decision Tree Classifier] Snap ML vs. Scikit-Learn speedup : {0:.2f}x '.format(training_speedup))\n",
        "\n",
        "# run inference and compute the probabilities of the test samples\n",
        "# to belong to the class of fraudulent transactions\n",
        "sklearn_pred = sklearn_dt.predict_proba(X_test)[:,1]\n",
        "\n",
        "# evaluate the Compute Area Under the Receiver Operating Characteristic\n",
        "# Curve (ROC-AUC) score from the predictions\n",
        "sklearn_roc_auc = roc_auc_score(y_test, sklearn_pred)\n",
        "print('[Scikit-Learn] ROC-AUC score : {0:.3f}'.format(sklearn_roc_auc))\n",
        "\n",
        "# run inference and compute the probabilities of the test samples\n",
        "# to belong to the class of fraudulent transactions\n",
        "snapml_pred = snapml_dt.predict_proba(X_test)[:,1]\n",
        "\n",
        "# evaluate the Compute Area Under the Receiver Operating Characteristic\n",
        "# Curve (ROC-AUC) score from the prediction scores\n",
        "snapml_roc_auc = roc_auc_score(y_test, snapml_pred)\n",
        "print('[Snap ML] ROC-AUC score : {0:.3f}'.format(snapml_roc_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZKvCLalPeXU"
      },
      "source": [
        "As shown above both decision tree models provide the same score on the test dataset. However Snap ML runs the training routine 12x faster than Scikit-Learn. This is one of the advantages of using Snap ML: acceleration of training of classical machine learning models, such as linear and tree-based models. For more Snap ML examples, please visit [snapml-examples](https://ibm.biz/BdPfxP?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork1047-2022-01-01).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgCNFszGPeXV"
      },
      "source": [
        "<div id=\"svm_sklearn\">\n",
        "    <h2>Build a Support Vector Machine model with Scikit-Learn</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5lF4EFLPeXV"
      },
      "outputs": [],
      "source": [
        "# import the linear Support Vector Machine (SVM) model from Scikit-Learn\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# instatiate a scikit-learn SVM model\n",
        "# to indicate the class imbalance at fit time, set class_weight='balanced'\n",
        "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
        "sklearn_svm = LinearSVC(class_weight='balanced', random_state=31, loss=\"hinge\", fit_intercept=False)\n",
        "\n",
        "# train a linear Support Vector Machine model using Scikit-Learn\n",
        "t0 = time.time()\n",
        "sklearn_svm.fit(X_train, y_train)\n",
        "sklearn_time = time.time() - t0\n",
        "print(\"[Scikit-Learn] Training time (s):  {0:.2f}\".format(sklearn_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzJUpsBxPeXe"
      },
      "source": [
        "<div id=\"svm_snap\">\n",
        "    <h2>Build a Support Vector Machine model with Snap ML</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-DTKJUXPeXe"
      },
      "outputs": [],
      "source": [
        "# import the Support Vector Machine model (SVM) from Snap ML\n",
        "from snapml import SupportVectorMachine\n",
        "\n",
        "# in contrast to scikit-learn's LinearSVC, Snap ML offers multi-threaded CPU/GPU training of SVMs\n",
        "# to use the GPU, set the use_gpu parameter to True\n",
        "# snapml_svm = SupportVectorMachine(class_weight='balanced', random_state=25, use_gpu=True, fit_intercept=False)\n",
        "\n",
        "# to set the number of threads used at training time, one needs to set the n_jobs parameter\n",
        "snapml_svm = SupportVectorMachine(class_weight='balanced', random_state=25, n_jobs=4, fit_intercept=False)\n",
        "# print(snapml_svm.get_params())\n",
        "\n",
        "# train an SVM model using Snap ML\n",
        "t0 = time.time()\n",
        "model = snapml_svm.fit(X_train, y_train)\n",
        "snapml_time = time.time() - t0\n",
        "print(\"[Snap ML] Training time (s):  {0:.2f}\".format(snapml_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4jb2sgnPeXe"
      },
      "source": [
        "<div id=\"svm_sklearn_snap\">\n",
        "    <h2>Evaluate the Scikit-Learn and Snap ML Support Vector Machine Models</h2>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8OufqS7PeXe"
      },
      "outputs": [],
      "source": [
        "# compute the Snap ML vs Scikit-Learn training speedup\n",
        "training_speedup = sklearn_time/snapml_time\n",
        "print('[Support Vector Machine] Snap ML vs. Scikit-Learn training speedup : {0:.2f}x '.format(training_speedup))\n",
        "\n",
        "# run inference using the Scikit-Learn model\n",
        "# get the confidence scores for the test samples\n",
        "sklearn_pred = sklearn_svm.decision_function(X_test)\n",
        "\n",
        "# evaluate accuracy on test set\n",
        "acc_sklearn  = roc_auc_score(y_test, sklearn_pred)\n",
        "print(\"[Scikit-Learn] ROC-AUC score:   {0:.3f}\".format(acc_sklearn))\n",
        "\n",
        "# run inference using the Snap ML model\n",
        "# get the confidence scores for the test samples\n",
        "snapml_pred = snapml_svm.decision_function(X_test)\n",
        "\n",
        "# evaluate accuracy on test set\n",
        "acc_snapml  = roc_auc_score(y_test, snapml_pred)\n",
        "print(\"[Snap ML] ROC-AUC score:   {0:.3f}\".format(acc_snapml))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq17MTc3PeXe"
      },
      "source": [
        "As shown above both SVM models provide the same score on the test dataset. However, as in the case of decision trees, Snap ML runs the training routine faster than Scikit-Learn. For more Snap ML examples, please visit [snapml-examples](https://ibm.biz/BdPfxP?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork1047-2022-01-01). Moreover, as shown above, not only is Snap ML seemlessly accelerating scikit-learn applications, but the library's Python API is also compatible with scikit-learn metrics and data preprocessors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4zbqNHCPeXe"
      },
      "source": [
        "### Practice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhb_EYx2PeXf"
      },
      "source": [
        "In this section you will evaluate the quality of the SVM models trained above using the [hinge loss metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html). Run inference on the test set using both Scikit-Learn and Snap ML models. Compute the hinge loss metric for both sets of predictions. Print the hinge losses of Scikit-Learn and Snap ML.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFtswJnUPeXf"
      },
      "outputs": [],
      "source": [
        "# get the confidence scores for the test samples\n",
        "sklearn_pred = sklearn_svm.decision_function(X_test)\n",
        "snapml_pred  = snapml_svm.decision_function(X_test)\n",
        "\n",
        "# import the hinge_loss metric from scikit-learn\n",
        "from sklearn.metrics import hinge_loss\n",
        "\n",
        "# evaluate the hinge loss from the predictions\n",
        "loss_snapml = hinge_loss(y_test, snapml_pred)\n",
        "print(\"[Snap ML] Hinge loss:   {0:.3f}\".format(loss_snapml))\n",
        "\n",
        "# evaluate the hinge loss metric from the predictions\n",
        "loss_sklearn = hinge_loss(y_test, sklearn_pred)\n",
        "print(\"[Scikit-Learn] Hinge loss:   {0:.3f}\".format(loss_snapml))\n",
        "\n",
        "# the two models should give the same Hinge loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFOK3MF4TS38"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from snapml import RandomForestClassifier, LogisticRegression, SupportVectorMachine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, hinge_loss, accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "\n",
        "# Train SVDD (emulated using SVM with Snap ML)\n",
        "svdd_svm = SupportVectorMachine(class_weight='balanced', random_state=25, n_jobs=4, fit_intercept=False)\n",
        "t0 = time.time()\n",
        "svdd_svm.fit(X_train, y_train)\n",
        "svdd_time = time.time() - t0\n",
        "print(\"[Snap ML - SVDD] Training time (s): {0:.2f}\".format(svdd_time))\n",
        "\n",
        "# Evaluate SVDD\n",
        "svdd_pred = svdd_svm.decision_function(X_test)\n",
        "svdd_pred_binary = (svdd_pred > 0).astype(int)\n",
        "roc_auc_svdd = roc_auc_score(y_test, svdd_pred)\n",
        "acc_svdd = accuracy_score(y_test, svdd_pred_binary)\n",
        "prec_svdd = precision_score(y_test, svdd_pred_binary)\n",
        "rec_svdd = recall_score(y_test, svdd_pred_binary)\n",
        "f1_svdd = f1_score(y_test, svdd_pred_binary)\n",
        "print(\"[Snap ML - SVDD] ROC-AUC score: {0:.3f}\".format(roc_auc_svdd))\n",
        "print(\"[Snap ML - SVDD] Accuracy: {0:.3f}\".format(acc_svdd))\n",
        "print(\"[Snap ML - SVDD] Precision: {0:.3f}\".format(prec_svdd))\n",
        "print(\"[Snap ML - SVDD] Recall: {0:.3f}\".format(rec_svdd))\n",
        "print(\"[Snap ML - SVDD] F1 Score: {0:.3f}\".format(f1_svdd))\n",
        "hinge_loss_svdd = hinge_loss(y_test, svdd_pred)\n",
        "print(\"[Snap ML - SVDD] Hinge loss: {0:.3f}\".format(hinge_loss_svdd))\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=25, n_jobs=4)\n",
        "t0 = time.time()\n",
        "rf.fit(X_train, y_train)\n",
        "rf_time = time.time() - t0\n",
        "print(\"[Snap ML - RF] Training time (s): {0:.2f}\".format(rf_time))\n",
        "\n",
        "# Evaluate Random Forest\n",
        "rf_pred = rf.predict_proba(X_test)[:, 1]\n",
        "rf_pred_binary = rf.predict(X_test)\n",
        "roc_auc_rf = roc_auc_score(y_test, rf_pred)\n",
        "acc_rf = accuracy_score(y_test, rf_pred_binary)\n",
        "prec_rf = precision_score(y_test, rf_pred_binary)\n",
        "rec_rf = recall_score(y_test, rf_pred_binary)\n",
        "f1_rf = f1_score(y_test, rf_pred_binary)\n",
        "print(\"[Snap ML - RF] ROC-AUC score: {0:.3f}\".format(roc_auc_rf))\n",
        "print(\"[Snap ML - RF] Accuracy: {0:.3f}\".format(acc_rf))\n",
        "print(\"[Snap ML - RF] Precision: {0:.3f}\".format(prec_rf))\n",
        "print(\"[Snap ML - RF] Recall: {0:.3f}\".format(rec_rf))\n",
        "print(\"[Snap ML - RF] F1 Score: {0:.3f}\".format(f1_rf))\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr = LogisticRegression(class_weight='balanced', random_state=25, n_jobs=4)\n",
        "t0 = time.time()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_time = time.time() - t0\n",
        "print(\"[Snap ML - LR] Training time (s): {0:.2f}\".format(lr_time))\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "lr_pred = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# If you need a decision function-like output, you can manually compute it from probabilities:\n",
        "# lr_pred = np.log(lr_pred / (1 - lr_pred))  # This approximates the decision function\n",
        "\n",
        "# The rest of your code should work as expected after this change.\n",
        "lr_pred_binary = (lr_pred > 0.5).astype(int)  # Adjust the threshold if needed\n",
        "roc_auc_lr = roc_auc_score(y_test, lr_pred)\n",
        "acc_lr = accuracy_score(y_test, lr_pred_binary)\n",
        "prec_lr = precision_score(y_test, lr_pred_binary)\n",
        "rec_lr = recall_score(y_test, lr_pred_binary)\n",
        "f1_lr = f1_score(y_test, lr_pred_binary)\n",
        "print(\"[Snap ML - LR] ROC-AUC score: {0:.3f}\".format(roc_auc_lr))\n",
        "print(\"[Snap ML - LR] Accuracy: {0:.3f}\".format(acc_lr))\n",
        "print(\"[Snap ML - LR] Precision: {0:.3f}\".format(prec_lr))\n",
        "print(\"[Snap ML - LR] Recall: {0:.3f}\".format(rec_lr))\n",
        "print(\"[Snap ML - LR] F1 Score: {0:.3f}\".format(f1_lr))\n",
        "hinge_loss_lr = hinge_loss(y_test, lr_pred)\n",
        "print(\"[Snap ML - LR] Hinge loss: {0:.3f}\".format(hinge_loss_lr))\n",
        "\n",
        "# Train K-Nearest Neighbors (using Scikit-Learn as Snap ML does not support KNN)\n",
        "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=4)\n",
        "t0 = time.time()\n",
        "knn.fit(X_train, y_train)\n",
        "knn_time = time.time() - t0\n",
        "print(\"[Scikit-Learn - KNN] Training time (s): {0:.2f}\".format(knn_time))\n",
        "\n",
        "# Evaluate K-Nearest Neighbors\n",
        "knn_pred = knn.predict_proba(X_test)[:, 1]\n",
        "knn_pred_binary = knn.predict(X_test)\n",
        "roc_auc_knn = roc_auc_score(y_test, knn_pred)\n",
        "acc_knn = accuracy_score(y_test, knn_pred_binary)\n",
        "prec_knn = precision_score(y_test, knn_pred_binary)\n",
        "rec_knn = recall_score(y_test, knn_pred_binary)\n",
        "f1_knn = f1_score(y_test, knn_pred_binary)\n",
        "print(\"[Scikit-Learn - KNN] ROC-AUC score: {0:.3f}\".format(roc_auc_knn))\n",
        "print(\"[Scikit-Learn - KNN] Accuracy: {0:.3f}\".format(acc_knn))\n",
        "print(\"[Scikit-Learn - KNN] Precision: {0:.3f}\".format(prec_knn))\n",
        "print(\"[Scikit-Learn - KNN] Recall: {0:.3f}\".format(rec_knn))\n",
        "print(\"[Scikit-Learn - KNN] F1 Score: {0:.3f}\".format(f1_knn))\n",
        "\n",
        "# Train Support Vector Machine (Scikit-Learn)\n",
        "svm = SVC(kernel='linear', probability=True, random_state=25)\n",
        "t0 = time.time()\n",
        "svm.fit(X_train, y_train)\n",
        "svm_time = time.time() - t0\n",
        "print(\"[Scikit-Learn - SVM] Training time (s): {0:.2f}\".format(svm_time))\n",
        "\n",
        "# Evaluate Support Vector Machine\n",
        "svm_pred = svm.predict_proba(X_test)[:, 1]\n",
        "svm_pred_binary = svm.predict(X_test)\n",
        "roc_auc_svm = roc_auc_score(y_test, svm_pred)\n",
        "acc_svm = accuracy_score(y_test, svm_pred_binary)\n",
        "prec_svm = precision_score(y_test, svm_pred_binary)\n",
        "rec_svm = recall_score(y_test, svm_pred_binary)\n",
        "f1_svm = f1_score(y_test, svm_pred_binary)\n",
        "print(\"[Scikit-Learn - SVM] ROC-AUC score: {0:.3f}\".format(roc_auc_svm))\n",
        "print(\"[Scikit-Learn - SVM] Accuracy: {0:.3f}\".format(acc_svm))\n",
        "print(\"[Scikit-Learn - SVM] Precision: {0:.3f}\".format(prec_svm))\n",
        "print(\"[Scikit-Learn - SVM] Recall: {0:.3f}\".format(rec_svm))\n",
        "print(\"[Scikit-Learn - SVM] F1 Score: {0:.3f}\".format(f1_svm))\n",
        "\n",
        "# Train Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=25)\n",
        "t0 = time.time()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_time = time.time() - t0\n",
        "print(\"[Scikit-Learn - DT] Training time (s): {0:.2f}\".format(dt_time))\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "dt_pred = dt.predict_proba(X_test)[:, 1]\n",
        "dt_pred_binary = dt.predict(X_test)\n",
        "roc_auc_dt = roc_auc_score(y_test, dt_pred)\n",
        "acc_dt = accuracy_score(y_test, dt_pred_binary)\n",
        "prec_dt = precision_score(y_test, dt_pred_binary)\n",
        "rec_dt = recall_score(y_test, dt_pred_binary)\n",
        "f1_dt = f1_score(y_test, dt_pred_binary)\n",
        "print(\"[Scikit-Learn - DT] ROC-AUC score: {0:.3f}\".format(roc_auc_dt))\n",
        "print(\"[Scikit-Learn - DT] Accuracy: {0:.3f}\".format(acc_dt))\n",
        "print(\"[Scikit-Learn - DT] Precision: {0:.3f}\".format(prec_dt))\n",
        "print(\"[Scikit-Learn - DT] Recall: {0:.3f}\".format(rec_dt))\n",
        "print(\"[Scikit-Learn - DT] F1 Score: {0:.3f}\".format(f1_dt))\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nSummary of Model Performances:\")\n",
        "print(\"SVDD: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}, Hinge Loss: {5:.3f}\".format(roc_auc_svdd, acc_svdd, prec_svdd, rec_svdd, f1_svdd, hinge_loss_svdd))\n",
        "print(\"Random Forest: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}\".format(roc_auc_rf, acc_rf, prec_rf, rec_rf, f1_rf))\n",
        "print(\"Logistic Regression: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}, Hinge Loss: {5:.3f}\".format(roc_auc_lr, acc_lr, prec_lr, rec_lr, f1_lr, hinge_loss_lr))\n",
        "print(\"KNN: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}\".format(roc_auc_knn, acc_knn, prec_knn, rec_knn, f1_knn))\n",
        "print(\"SVM: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}\".format(roc_auc_svm, acc_svm, prec_svm, rec_svm, f1_svm))\n",
        "print(\"Decision Tree: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}\".format(roc_auc_dt, acc_dt, prec_dt, rec_dt, f1_dt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the ANN model**"
      ],
      "metadata": {
        "id": "Ubbv0phdMRPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data (ANNs often require input normalization for better convergence)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert labels to categorical if it's a multi-class problem (use to_categorical)\n",
        "# For binary classification, this step isn't needed as the ANN expects a single output node with a sigmoid activation\n",
        "# y_train_cat = to_categorical(y_train)\n",
        "# y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# Build the ANN model\n",
        "ann = Sequential([\n",
        "    Dense(128, activation='relu', input_dim=X_train.shape[1]),  # Input layer\n",
        "    Dropout(0.3),  # Dropout for regularization\n",
        "    Dense(64, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Output layer (sigmoid for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "ann.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the ANN\n",
        "t0 = time.time()\n",
        "history = ann.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "ann_time = time.time() - t0\n",
        "print(\"[TensorFlow - ANN] Training time (s): {0:.2f}\".format(ann_time))\n",
        "\n",
        "# Evaluate the ANN\n",
        "ann_pred_proba = ann.predict(X_test_scaled).flatten()  # Get probabilities\n",
        "ann_pred_binary = (ann_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "roc_auc_ann = roc_auc_score(y_test, ann_pred_proba)\n",
        "acc_ann = accuracy_score(y_test, ann_pred_binary)\n",
        "prec_ann = precision_score(y_test, ann_pred_binary)\n",
        "rec_ann = recall_score(y_test, ann_pred_binary)\n",
        "f1_ann = f1_score(y_test, ann_pred_binary)\n",
        "print(\"[TensorFlow - ANN] ROC-AUC score: {0:.3f}\".format(roc_auc_ann))\n",
        "print(\"[TensorFlow - ANN] Accuracy: {0:.3f}\".format(acc_ann))\n",
        "print(\"[TensorFlow - ANN] Precision: {0:.3f}\".format(prec_ann))\n",
        "print(\"[TensorFlow - ANN] Recall: {0:.3f}\".format(rec_ann))\n",
        "print(\"[TensorFlow - ANN] F1 Score: {0:.3f}\".format(f1_ann))\n",
        "\n",
        "# Print the updated summary\n",
        "print(\"\\nUpdated Summary of Model Performances:\")\n",
        "print(\"ANN: ROC-AUC: {0:.3f}, Accuracy: {1:.3f}, Precision: {2:.3f}, Recall: {3:.3f}, F1 Score: {4:.3f}\".format(\n",
        "    roc_auc_ann, acc_ann, prec_ann, rec_ann, f1_ann\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YEqoV9-rL6VM",
        "outputId": "ed9903b0-2df1-4974-9aae-6984621d71fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-822bc96ed3bc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Normalize the data (ANNs often require input normalization for better convergence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}